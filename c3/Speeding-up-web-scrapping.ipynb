{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53952a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem : Request a list of pages in parallel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0563fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "import requests\n",
    "import logging\n",
    "import http.client\n",
    "import re\n",
    "from urllib.parse import urlparse, urljoin \n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac49c3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-u U] [-p P] [-w W]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/jose/.local/share/jupyter/runtime/kernel-234ffe7a-ee02-4a07-9783-274990b3c836.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "URL = 'http://localhost:8000/'\n",
    "DEFAULT_PHRASE = 'python' \n",
    "\n",
    "# Process the links\n",
    "\n",
    "def process_link(source_link, text):\n",
    "    logging.info(f'Extracting links from {source_link}')\n",
    "    parsed_source = urlparse(source_link)\n",
    "    result = requests.get(source_link)\n",
    "    if results.status.code != http.client.OK:\n",
    "        logging.error(f'Error retrieving {source_link} : {result}')\n",
    "        return source_link, []\n",
    "    \n",
    "    if 'html' not in result.headers['Content-type']:\n",
    "        logging.info(f'Link {source_link} is not an HTML page')\n",
    "        return source_link, []\n",
    "    \n",
    "    page = BeautifulSoup(result.text, 'html.parser')\n",
    "    search_text(source_link, page, text)\n",
    "    \n",
    "    return source_link, get_links(parsed_source, page)\n",
    "\n",
    "# Get the links \n",
    "\n",
    "def get_links(parsed_source, page):\n",
    "    '''Retrieve the link from the page'''\n",
    "    links = []\n",
    "    for element in page.find_all('a'):\n",
    "        link = element.get('href')\n",
    "        if not link:\n",
    "            continue\n",
    "\n",
    "        if link.startswith('#'):\n",
    "            continue\n",
    "\n",
    "        if link.startswith('mailto'):\n",
    "            continue\n",
    "\n",
    "        # Always accept local links \n",
    "        if not link.startswith('http'):\n",
    "            netloc = parsed_source.netloc\n",
    "            scheme = parsed_source.scheme\n",
    "            path = urljoin(parsed_source.path, link)\n",
    "            link = f'{scheme}://{netloc}{path}'\n",
    "\n",
    "        # Only parse links in the same domain \n",
    "        if parsed_source.netloc not in link:\n",
    "            continue \n",
    "\n",
    "        links.append(link)\n",
    "\n",
    "    return links \n",
    "\n",
    "def search_text(source_link, page, text):\n",
    "    '''Search for an element with the searched text and print it'''\n",
    "    for element in page.find_all(text=re.compile(text, flags=re.IGNORECASE)):\n",
    "        print(f'Link {source_link} : --> {element}')\n",
    "        \n",
    "def main(base_url, to_search, workers):\n",
    "    checked_links = set()\n",
    "    to_check = [base_url]\n",
    "    max_checks = 10\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        while to_check:\n",
    "            futures = [executor.submit(process_link, url, to_search)\n",
    "                        for url in to_check]\n",
    "            to_check = []\n",
    "            for data in concurrent.futures.as_completed(futures):\n",
    "                link, new_links = data.result()\n",
    "                \n",
    "                checked_links.add(link)\n",
    "                for link in new_links:\n",
    "                    if Link not in checked_links and link not in to_check:\n",
    "                        to_check.append(link)\n",
    "                        \n",
    "                max_checks -= 1\n",
    "                if not max_checks:\n",
    "                    return \n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-u', type=str, help='Base site url',\n",
    "                        default=URL)\n",
    "    parser.add_argument('-p', type=str, help='Sentence to search',\n",
    "                        default=DEFAULT_PHRASE)\n",
    "    parser.add_argument('-w', type=int, help='Number of workers',\n",
    "                        default=4)\n",
    "    args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc4df21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
